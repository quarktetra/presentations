[["ReMan_model.html", "A Treatise on ReMan ReMan provides substantial benefits Modeling failure distributions Visualizing failure distributions Modeling One-Head ReMan Modeling k-Head ReMan ReMan eliminates head related failures ReMan preserves capacity at the edge ReMan reduces swap rate and TCO in data centers ReMan improves data durability Simulation with online ReMan Modeling variable failure rates References", " A Treatise on ReMan Serkay Ölmez May 18, 2021 Abstract The goal of this work is to develop a complete model to study the benefits of depopulating heads in the field, which is known as ReMan. These benefits include the extension of the device lifetime and the gain in the data durability for erasure coded systems. We will describe the devices with two distinct failure modes: the first one is head related, causing individual heads to fail, and the other will be at the device level, failing the whole device. We will also provide simulation results to support the model. .sliderlabel{ text-align:center; font-size:12px; margin-top: 0.7em; } ReMan provides substantial benefits The latest generation of HDDs have up to 20 heads: A significant portion of failures is head specific. The ReMan technology enables drives to continue to run even after a certain number of head failures. This practically eliminates head specific failures and reduces the overall failure rate by up to a factor of \\(5\\)X. Online ReMan enables substantial gains in data durability. Online ReMan enables much faster data recovery. For a 20-header, 20TB drive, only 1TB needs to be recovered rather than 20TB. The latest generation of HDDs has up to 20 heads. A significant portion of failures is head specific. The ReMan [ReManufacturing drives with head failures] technology enables drives to continue to run even after a certain number of head failures. This practically eliminates head specific failures and reduces the overall failure rate by up to a factor of \\(5\\)X.Online ReMan enables much faster data recovery. For a 20-header, 20TB drive, only 1TB needs to be recovered rather than 20TB. serkay.olmez@seagate.com Acknowledgments: Iman Anvari, Mike Barrell, John Bent, Drew Cheney, Ian Davies, Cary Johnson, Erin Foley, and Matt Shumway. Set the level of detail with the buttons above Press F11 for full screen Ctrl +/- to fit the content into the browser Use arrow keys to navigate the slides Table of contents button on the left bottom Printable PDF: This is a static copy of the presentation. Compiled with BookDown xie2015? and Revealjs revealjs? Fragments: Enable or disable fragment transitions within slides .mathjax-tooltip { display: none; width: 100%; position: absolute; background-color:gray; } Modeling failure distributions Weibull distribution [1] is a versatile function due to its flexibility to fit distributions of various shapes. We first consider a particular device failure mode, and assume it can be accurately fit by a Weibull distribution function. A Weibull distribution has two parameters: \\(\\alpha\\) parameter sets the time scale whereas the \\(\\beta\\) parameter sets the shape of the distribution. The failure probability density, cumulative failure distributions, and the hazard rate (failure rate) are defined as follows: \\[\\begin{equation} f_{\\alpha,\\beta}(t)=\\frac{\\beta }{\\alpha} \\left( \\frac{t}{\\alpha}\\right)^{\\beta -1} e^{-\\left( \\frac{t}{\\alpha}\\right)^{\\beta}},\\tag{1} \\end{equation}\\] \\[\\begin{equation} F_{\\alpha,\\beta}(t)= \\int_{0}^{t} d\\tau f_{\\alpha,\\beta}(\\tau)=1-e^{-\\left(\\frac{t}{\\alpha}\\right)^{\\beta}},\\tag{2} \\end{equation}\\] \\[\\begin{equation} h_{\\alpha,\\beta}(t)= \\frac{f_{\\alpha,\\beta}(t)}{1-F_{\\alpha,\\beta}(t)}=\\frac{\\beta }{\\alpha} \\left( \\frac{t}{\\alpha}\\right)^{\\beta -1}.\\tag{3} \\end{equation}\\] The hazard rate function, \\(h_{\\alpha,\\beta}(t)\\), is the event rate normalized with respect the to the population that survived until \\(t\\). The actual values of \\(\\alpha\\) and \\(\\beta\\) vary from product to product: \\(\\beta\\) is expected to be between \\(1\\) and \\(2\\). When \\(\\beta\\neq1\\), the failure rates are time dependent. It is certainly possible to model ReMan with \\(\\beta\\neq1\\), but \\(\\beta=1\\) is typically a good fit, and that is what we will assume for most of the modeling work here. The case of \\(\\beta\\neq1\\) is treated in the last section. except for the last section Visualizing failure distributions In the interactive plot below, \\(\\alpha\\) and \\(\\beta\\) values can be adjusted. The toggles on the right change the scales. \\(\\beta\\) \\(\\alpha\\) x:Lin/Log y:Lin/Prob Figure 1: An interactive plot of Weibull distributions defined in Eqs. (1) , (2) and (3). Use the toggles for linear/log scales.  [+] The vertical dashed lines mark the value \\(t=\\alpha\\), and the horizontal one on \\(F\\) marks the value \\(0.63\\). The \\(y\\) axis can be transformed to \\(ln\\left[-ln(1-F_{\\alpha,\\beta})\\right]\\), and the time axis can be transformed to \\(ln(t)\\). The effect of \\(\\beta\\) on \\(F\\) is best observed with the log time scale, and \\(y\\) in the probability scale: notice how \\(F_{\\alpha,\\beta}\\) becomes a line with the slope \\(\\beta\\). Selecting the log scale for time and linear scale for the \\(y\\) axis, shows that the changing scale parameter \\(\\alpha\\) moves the functions horizontally while preserving their shapes. The actual values of \\(\\alpha\\) and \\(\\beta\\) vary from product to product: \\(\\beta\\) is expected to be between \\(1\\) and \\(2\\). When \\(\\beta\\neq1\\), the failure rates are time dependent. It is certainly possible to model ReMan with \\(\\beta\\neq1\\), but \\(\\beta=1\\) is typically a good fit, and that is what we will assume in this work, except for the last chapter. \\(\\beta=1\\) gives the exponential distribution, which has completely random head failure times with a fixed failure(hazard) rate: \\(h_{\\alpha,\\beta}(t)=\\frac{1}{\\alpha}\\equiv\\lambda\\). This simplifies Eqs. (1) , (2) and (3) to: \\[\\begin{equation} f_{\\lambda}(t)= \\lambda e^{-\\lambda t},\\quad F_{\\lambda}(t)=1-e^{-\\lambda t},\\, \\rm{and}\\quad h_{\\lambda}(t)=\\lambda. \\tag{4} \\end{equation}\\] It is important to understand how the storage devices fail. Weibull is typically a good fit. No matter how reliable individual devices are, failures are inevitable. Therefore, redundancies are needed to preserve data in case of drive failures. Modeling One-Head ReMan The probability of having no failures in an \\(N\\)-header drive is: \\[\\begin{equation} p_{drive}^{pass}=p_{head}^{ all\\, pass}=\\left(p_{head}^{pass}\\right)^N=e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}, \\tag{5} \\end{equation}\\] where \\(\\alpha\\) and \\(\\beta\\) are head-level Weibull parameters. The probability of having exactly one head failure is \\[\\begin{eqnarray} p_{drive}^{ exactly\\,one\\, head \\,fail }&amp;=&amp;N\\,\\left(p_{head}^{fail}\\right)\\left(p_{head}^{pass}\\right)^{N-1}=N\\,\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right) e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{6} \\end{eqnarray}\\] Then the cumulative probability of having two or more failures in an \\(N\\)-header drive (\\(CDF_R\\)) reads: \\[\\begin{equation} %1-e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}-N\\,\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right) e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}} CDF_R=N \\left(1-e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)-(N-1)\\left(1-e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right). \\tag{7} \\end{equation}\\] Eq. (7) is a mixture of two Weibull modes. The first term describes \\(N-1\\) header drives, and the second describes \\(N\\) header drives. This equation still uses the time per head, and we need to convert that to drive level time. After the first head failure, the head time will increase as \\(t_d/(N-1)\\). Scaling the time in Eq. (7), we get \\[\\begin{eqnarray} CDF_R^{drive}(t_d)&amp;=&amp;1-p_{drive}^{pass}-p_{drive}^{ exactly\\,one\\, head \\,fail } = N \\left(1-e^{-(N-1)\\,\\left( \\frac{t_d}{\\alpha(N-1)}\\right)^{\\beta}}\\right) -(N-1)\\left(1-e^{-N\\,\\left( \\frac{t_d}{\\alpha(N-1)}\\right)^{\\beta}}\\right). \\tag{8} \\end{eqnarray}\\] Modeling k-Head ReMan Consider the case where at most \\(k\\)-head failure is acceptable. The probability of having up to and including \\(k\\) failures is \\[\\begin{eqnarray} p_{drive}^{ up\\, to\\, k \\, head \\,failures }&amp;=&amp;\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(p_{head}^{fail}\\right)^j\\left(p_{head}^{pass}\\right)^{N-j} =\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)^j e^{-(N-j)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{9} \\end{eqnarray}\\] Then the CDF for having more than \\(k\\) head failures becomes \\[\\begin{eqnarray} CDF(\\mathrm{k-ReMAN})&amp;=&amp;p_{drive}^{more\\, than\\, k \\, head\\,failures} =1-p_{drive}^{ up\\, to\\, k \\, head \\,failures }\\nonumber\\\\ &amp;=&amp;1-\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)^j e^{-(N-j)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{10} \\end{eqnarray}\\] ReMan eliminates head related failures There are two types of failures: Head specific failures, which can be handled by ReMan: Just depop the failing head, and move on. We will show that this practically eliminates head related failures. Entire device failures: motor/PCBA/HSA failures, etc. These cannot be addressed by ReMan. The overall reliability of a device can be computed by the multiplying the individual reliability functions, \\(R_i\\): \\[\\begin{equation} R(t)={\\displaystyle \\prod_{i}}R_i= R_\\text{ReMan&#39;able}\\times R_\\text{non-ReMan&#39;able}. \\tag{11} \\end{equation}\\] \\(R_\\text{non-ReMan&#39;able}\\) is the reliability associated with the entire device failures, and it can simply be defined by a failure rate \\(\\lambda_\\text{NR}\\). \\(R_\\text{ReMan&#39;able}\\), on the other hand, is the associated with the head-specific failures. They are fixable by depoping heads. \\(R_\\text{ReMan&#39;able}\\) can be derived from Eq. (8) as \\(1-CDF_R^{drive}(t_d)\\). See Fig. 2 for an example simulation and the caption for the parameters. See Fig. ?? for an example simulation. See the caption for the parameters. Show the ReMan simulation script (R) Hide library(dplyr);library(plotly);library(rstudioapi); # A code to simulate reliability with ReMan. Last update on 5/17/2021, serkay.olmez@seagate.com Nh=18; # number of heads per drive ndr=100000; #number of drives to simulate ### Reman&#39;able Failure mode parameters beta=1; afr=0.8 # in pct alpha&lt;-round(8760/(-log(1-afr/100))^(1/beta)) digits&lt;-floor(log10(alpha)) ### Non-Reman&#39;able Failure mode parameters betaNR=1; afrNR=0.2 # in pct alphaNR&lt;-round(8760/(-log(1-afrNR/100))^(1/betaNR)) head_failure_time&lt;-rweibull(Nh*ndr, shape=beta, scale=alpha) # pull Nh* ndr weibull numbers hdid&lt;-c(0:(Nh*ndr-1)); # create unique head id drvid&lt;-floor(hdid/Nh) # group heads into drives fdf&lt;-data.frame(&quot;drvid&quot;=drvid,&quot;hdid&quot;=hdid,&quot;head_failure_time&quot;=head_failure_time)# putting the columns into a dataframe fdf&lt;-fdf[ with(fdf, order(drvid,head_failure_time)), ] mindf&lt;-fdf%&gt;% group_by(drvid) %&gt;% summarise (hft1= min(head_failure_time))# locating the first head failure mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindf, by=&quot;drvid&quot;, all.x = TRUE) # joining it back into the data frame. fdfs&lt;-fdf[!fdf$hft1==fdf$head_failure_time,] mindfs&lt;-fdfs%&gt;% group_by(drvid) %&gt;% summarise (hft2= min(head_failure_time)) mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindfs, by=&quot;drvid&quot;, all.x = TRUE) fdf$drv_ft_no_Reman&lt;-Nh*fdf$hft1;fdf$drv_ft_one_Reman&lt;-(Nh-1)*fdf$hft2+fdf$hft1 fdfs&lt;-fdf[fdf$head_failure_time&gt;fdf$hft2,] mindfs&lt;-fdfs%&gt;% group_by(drvid) %&gt;% summarise (hft3= min(head_failure_time)) mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindfs, by=&quot;drvid&quot;, all.x = TRUE) fdf$drv_ft_two_Reman&lt;-(Nh-2)*fdf$hft3+fdf$hft2+fdf$hft1 dfsum&lt;-fdf%&gt;% group_by(drvid)%&gt;% summarise (drive_ft_no_Reman= min(drv_ft_no_Reman),drive_ft_one_Reman= min(drv_ft_one_Reman),drive_ft_two_Reman= min(drv_ft_two_Reman)) dfsum&lt;-as.data.frame(dfsum) dfsum$driveNR_time&lt;-rweibull(ndr, shape=betaNR, scale=alphaNR) # pull Nh* ndr weibull numbers dfsum &lt;- transform(dfsum, drv_failure_time_one_Reman = pmin(drive_ft_one_Reman, driveNR_time)) dfsum &lt;- transform(dfsum, drv_failure_time_two_Reman = pmin(drive_ft_two_Reman, driveNR_time)) dfsum &lt;- transform(dfsum, drv_failure_time_no_Reman = pmin(drive_ft_no_Reman, driveNR_time)) stepsize&lt;-min(round(alpha/10000),round(alphaNR/10000)) dfsum$time&lt;-stepsize*round(dfsum$drv_failure_time_one_Reman/stepsize,0) dfsums&lt;-dfsum%&gt;%group_by(time) %&gt;% summarise(count=n()) dfsums&lt;-as.data.frame(dfsums);dfsums$density&lt;-dfsums$count/ndr dfsums &lt;- within(dfsums, cdf_one_Reman &lt;- cumsum(density)) dfsums$cdf_one_Reman&lt;- 100*dfsums$cdf_one_Reman dfsum$timez&lt;-stepsize*round(dfsum$drv_failure_time_no_Reman/stepsize,0) dfsumsz&lt;-dfsum%&gt;% group_by(timez) %&gt;% summarise(countz=n()) dfsumsz&lt;-as.data.frame(dfsumsz);dfsumsz$densityz&lt;-dfsumsz$countz/ndr dfsumsz &lt;- within(dfsumsz, cdf_no_Reman &lt;- cumsum(densityz)) dfsumsz$cdf_no_Reman&lt;- 100*dfsumsz$cdf_no_Reman dfsums&lt;-merge(x = dfsums, y = dfsumsz[,c(&quot;timez&quot;,&quot;cdf_no_Reman&quot;)], by.x=&quot;time&quot;, by.y=&quot;timez&quot;,all.x = TRUE) dfsum$timez&lt;-stepsize*round(dfsum$drv_failure_time_two_Reman/stepsize,0) dfsumsz&lt;-dfsum%&gt;% group_by(timez) %&gt;% summarise(countz=n()) dfsumsz&lt;-as.data.frame(dfsumsz);dfsumsz$densityz&lt;-dfsumsz$countz/ndr dfsumsz &lt;- within(dfsumsz, cdf_two_Reman &lt;- cumsum(densityz)) dfsumsz$cdf_two_Reman&lt;- 100*dfsumsz$cdf_two_Reman dfsums&lt;-merge(x = dfsums, y = dfsumsz[,c(&quot;timez&quot;,&quot;cdf_two_Reman&quot;)], by.x=&quot;time&quot;, by.y=&quot;timez&quot;,all.x = TRUE) dfsums$cdf_no_Reman_Theory&lt;-100*(1-exp(-(Nh)*((dfsums$time/alpha)/(Nh))^beta-(dfsums$time/alphaNR)^beta)) dfsums$cdf_one_Reman_Theory&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta) * (exp(-Nh*((dfsums$time/alpha)/(Nh))^beta) +Nh*(1-exp(-((dfsums$time/alpha)/(Nh))^beta)) *exp(-(Nh-1)*((dfsums$time/alpha)/(Nh))^beta))) dfsums$cdf_two_Reman_Theory&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta)* (exp(-Nh*((dfsums$time/alpha)/(Nh))^beta) +Nh*(1-exp(-((dfsums$time/alpha)/(Nh))^beta))*exp(-(Nh-1) *((dfsums$time/alpha)/(Nh))^beta) +Nh*(Nh-1)/2*(1-exp(-((dfsums$time/alpha)/(Nh))^beta))^2 *exp(-(Nh-2)*((dfsums$time/alpha)/(Nh))^beta)) ) dfsums$cdf_Ratio_Theory&lt;-dfsums$cdf_no_Reman_Theory/dfsums$cdf_one_Reman_Theory dfsums$dtimey&lt;-dfsums$time*Nh dfsums$time_year&lt;-dfsums$time/8760 dfsums$beta&lt;-beta;dfsums$betaNR&lt;-betaNR dfsums$alpha&lt;-alpha;dfsums$alphaNR&lt;-alphaNR dfsums$afr&lt;-afr;dfsums$afrNR&lt;-afrNR dfsums$Nh&lt;-Nh;# number of heads per drive dfsums$Ndr&lt;-ndr dfsums$floor&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta)) #$sourcecode[1]&lt;-rstudioapi::getActiveDocumentContext()$path dfsums$creationdate[1]&lt;-Sys.time() desiredval=1; time_index=which(abs(dfsums$time_year-desiredval)==min(abs(dfsums$time_year-desiredval))) desiredval2=5; time_index2=which(abs(dfsums$time_year-desiredval2)==min(abs(dfsums$time_year-desiredval2))) #dfsums$cdf_no_Reman[time_index] start_point_x&lt;-c(desiredval,desiredval2,desiredval+0.35,desiredval2-0.35) start_point_y&lt;-c(dfsums$cdf_no_Reman[time_index],dfsums$cdf_no_Reman[time_index2],(dfsums$cdf_no_Reman[time_index]+dfsums$cdf_one_Reman[time_index])/2,(dfsums$cdf_no_Reman[time_index2]+dfsums$cdf_one_Reman[time_index2])/2) end_point_x&lt;-c(desiredval,desiredval2,desiredval+0.15,desiredval2-0.15) end_point_y&lt;-c(dfsums$cdf_one_Reman[time_index],dfsums$cdf_one_Reman[time_index2],(dfsums$cdf_no_Reman[time_index]+dfsums$cdf_one_Reman[time_index])/2,(dfsums$cdf_no_Reman[time_index2]+dfsums$cdf_one_Reman[time_index2])/2) atext&lt;-c(&quot;&quot;,&quot;&quot;,paste0(round(dfsums$cdf_no_Reman[time_index]/dfsums$cdf_one_Reman[time_index]),&quot;X reduction&quot;),paste0(round(dfsums$cdf_no_Reman[time_index2]/dfsums$cdf_one_Reman[time_index2]),&quot;X reduction&quot;)) a &lt;- list( ax = start_point_x, ay = start_point_y, x=end_point_x, y=end_point_y, text = atext, xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = TRUE, arrowhead = 3, xshift=0, yshift=0, axref=&#39;x&#39;, ayref=&#39;y&#39; ) anny=c(1,2) annx=c(1.5,1) t &lt;- list( size = 14) ay &lt;- list( tickfont = list(color = &quot;black&quot;), overlaying = &quot;y&quot;,side = &quot;right&quot;,title = &quot;CFR_gain&quot;) pcfr&lt;-plot_ly(dfsums,type = &#39;scatter&#39;) %&gt;% add_trace(y = ~cdf_no_Reman_Theory,x = ~time_year, name = &#39;CDF(no ReMan-calculation)&#39;, mode = &#39;lines&#39;) %&gt;% #, visible=&quot;legendonly&quot; add_trace(y = ~cdf_one_Reman_Theory,x = ~time_year, name = &#39;CDF(one ReMan-calculation)&#39;, mode = &#39;lines&#39;) %&gt;% #, visible=&quot;legendonly&quot; add_trace(y = ~cdf_one_Reman,x = ~time_year, name = &#39;CDF(one ReMan-simulation)&#39;, mode = &#39;markers+lines&#39;) %&gt;% add_trace(y = ~cdf_no_Reman,x = ~time_year,name = &#39;CDF(no ReMan-simulation)&#39;, type = &#39;scatter&#39;, mode = &#39;markers+lines&#39;) %&gt;% #add_trace(y = ~anny,x=~annx, name = &#39;d&#39;, mode = &#39;lines&#39;, line=list(color=&#39;gray&#39;,dash=&#39;dash&#39;),showlegend=FALSE) %&gt;% #add_trace(y = ~cdf_two_Reman, name = &#39;CDF_two_Reman_Simulation&#39;, mode = &#39;markers+lines&#39;) %&gt;% #add_trace(y = ~cdf_two_Reman_Theory, name = &#39;CDF_two_Reman_Theory&#39;, mode = &#39;markers+lines&#39;) %&gt;% #add_trace(y = ~floor, name = &#39;CDF_Non-Remanable&#39;, mode = &#39;lines&#39;, line=list(dash=&quot;dash&quot;)) %&gt;% layout(annotations = a,legend = list(x = 0.2, y = 0.95))%&gt;% layout( yaxis2 = ay,xaxis = list(title=&quot;time (years)&quot;, range = c(0,5), hoverformat= &#39;.1f&#39;), yaxis = list(title=&quot;Cumulative Failures(%)&quot;,range = c(-0.1,5.5), hoverformat= &#39;.2f&#39;) )%&gt;% layout(margin=list(r = 80,l = 80,t=40,b = 40), yaxis=list(tickprefix=&quot; &quot;))%&gt;% layout(hovermode = &#39;compare&#39;)%&gt;% layout(title=&quot;Cumulative Failures with and without Reman&quot;, font=t) #%&gt;%add_lines(y=~cdf_Ratio_Theory, name = &quot;CFR_gain&quot;, yaxis = &quot;y2&quot;) pcfr Figure 2: Comparison of cumulative failures with and without ReMan. Total failure rate is taken as \\(1\\%\\) split into \\(0.8\\%\\) ReManable and \\(0.2\\%\\) Non-ReManable.  [+] We also take \\(\\beta_{R}=1\\), and \\(\\beta_{NR}=1.\\) Click on the legend to display the hidden traces. Even one-head ReMan absorbs practically all head specific failures. In this example, the Annualized Failure Rate (AFR) is effectively reduced down to \\(0.2\\%\\) from the original value of \\(1\\%\\). See Fig.5 for an interactive plot to adjust the total AFR and its split into the two categories. ReMan preserves capacity at the edge Consider an edge application in which failed drives are not replaced immediately. ReMan will be an indispensable tool to preserve the capacity of the storage system: Entire device failures will take down the full drive causing the loss of the full capacity. Head specific failures will be ReManed, losing \\(1/N\\) of the full cap (\\(5\\%\\) for a 20-header drive). ReMan helps preserve capacity much longer. Details on the capacity computations Collapse When no ReMan is allowed the average capacity will drop with drive failures, which can be written as follows: \\[\\begin{equation} \\mathcal{CAP}_{0R}=\\mathcal{CAP}_0(1-CDF_{0R}) \\tag{12} \\end{equation}\\] When at most one ReMan is allowed, the average capacity will be \\[\\begin{equation} \\mathcal{CAP}_{1R}=\\mathcal{CAP}_0(1-CDF_{1R}) -\\mathcal{CAP}_0(CDF_{0R}-CDF_{1R})/N, \\tag{13} \\end{equation}\\] where the second term is the ratio of the drives with exactly one head failure [they lost a capacity of \\(\\mathcal{CAP}_0/N\\)]. Figure 3 shows the CDF and capacity. \\(\\text{Total Failure Rate}(\\%)\\) \\(\\text{Percentage of Reman&#39;able Failure Rate}(\\%)\\) y:Lin/Log CDF/CAP Time(yr) CDF no ReMan CDF one-head ReMan Avg. CAP no ReMan Avg. CAP one-head ReMan Figure 3: vs time with Number of heads \\(N=20\\). Note the switches to toggle the y-axis between linear vs log scale and between CDF vs Capacity.  [+] We also take \\(\\beta_{R}=1\\), and \\(\\beta_{NR}=1.\\) ReMan reduces swap rate and TCO in data centers As shown earlier, ReMan substantially reduces drive failures and, in turn, the cost of replacing them. The only drawback of ReMan is that ReManed drives will have one-fewer heads, hence slightly reduced capacity. In the case of maximum one-head ReMan, the overall capacity of the system will reduce only slightly. Qualitatively speaking, the ratio of ReManed drives will increase slowly, because the increase is suppressed by two factors: As drives fail for the NonReManable mode, they are replaced. This removes already ReManed drives out of the population. If a second head in an already-ReManed drive fails, the drive is replaced. This also removes ReManed drives out of the population. The precise mathematical model is not trivial[2] due to the factors listed above. The final expression for the ReManed drive density is given in Eq. (18) and it is plotted and compared with simulation results in Figure 4 for illustation purposes[see the caption for details]. Once the ReManed drive density is calculated, the degradation in the overall capacity can be computed as in Eq. (20) . The change of average capacity with time is shown in Fig. 5. See Fig. 5 to adjust the inputs to the model to observe how the average capacity changes. Also note that the time entry in the table below the plot is editable. Details on the computations Collapse When a head fails, it is turned off, and the drive continues to operate with reduced capacity. We will denote the ratio of devices that lost \\(i\\) heads as \\(r_i\\), and compute how these ratios evolve with time. We first consider a replacement policy which allows for maximum one-head ReMan, which implies that \\[\\begin{eqnarray} r_0(t)+r_1(t)=1.\\tag{14} \\end{eqnarray}\\] Furthermore, since the system starts with new devices, the initial values of the ratios are \\[\\begin{eqnarray} r_0(0)=1 \\quad \\text{and}\\quad r_1(0)=0.\\tag{15} \\end{eqnarray}\\] We need to calculate how \\(r_0\\) evolves with time given that devices have two distinct failure modes: i) a failure mode that affects individual heads, ii) another failure mode that affects the whole device and results in device replacement. We define the failure rate that affects heads as \\(\\lambda_\\mathrm{R}\\), where R stands for Re-manufacturable, and the other mode as \\(\\lambda_\\mathrm{NR}\\), where NR stands for NonRe-manufacturable. As heads fail, the ratio of the devices with no head failures (\\(r_0\\)) will decrease: \\(\\dot r_0\\propto -\\lambda_\\mathrm{R}\\, r_0\\). On the other hand, devices that already have one head failure (\\(r_1\\)) will be replaced with new devices once another head fails. The replacement will move these devices to the \\(r_0\\) bucket. The rate of this event is proportional to the total number of heads in \\(r_1\\) bucket. Adding this term yields \\(\\dot r_0\\propto -\\lambda_\\mathrm{R}\\, r_0+ \\lambda_\\mathrm{R}\\,\\frac{N-1}{N}\\, r_1\\), where the factor \\(\\frac{N-1}{N}\\) normalizes the failure rate since there are \\(N-1\\) heads per device in the \\(r_1\\) bucket. There is one more term to add: the device level failures with rate \\(\\lambda_\\mathrm{NR}\\). Such failures require device replacements, increasing \\(r_0\\) only if the failure happens to be in the \\(r_1\\) bucket. Therefore the rate of such events will be \\(r_1 \\lambda_\\mathrm{NR}\\). Adding all the terms, and using \\(r_1=1-r_0\\) result in the following differential equation: \\[\\begin{eqnarray} \\dot r_0&amp;=&amp; -\\lambda_\\mathrm{R}\\, r_0+\\lambda_\\mathrm{R}\\,\\frac{N-1}{N}(1-r_0)+ \\lambda_\\mathrm{NR} (1-r_0). \\tag{16} \\end{eqnarray}\\] Eq. (16) has the following solution: \\[\\begin{equation} r_0(t)= \\frac{N-1+ N\\kappa+N e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}}{2N-1+N\\kappa }=1-\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}\\right), \\tag{17} \\end{equation}\\] where we defined \\(\\kappa=\\frac{\\lambda_\\mathrm{NR}}{\\lambda_\\mathrm{R}}\\). Since \\(r_0+r_1=1\\), we get \\[\\begin{equation} r_1(t)=1-r_0(t)=\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}\\right), \\tag{18} \\end{equation}\\] which is the relative ratio of the ReManed drives. We can now write the average capacity of the system: \\[\\begin{equation} \\overline{CAP}=C(r_0+r_1\\frac{N-1}{N})=C(r_0+r_1-\\frac{r_1}{N})=C(1-\\frac{r_1}{N}) \\tag{19} \\end{equation}\\] Equivalently, we can define the relative the loss in the capacity: \\[\\begin{equation} \\frac{\\Delta\\overline{CAP}}{C}=\\frac{C-\\overline{CAP}}{C}=\\frac{r_1}{N}, \\tag{20} \\end{equation}\\] which basically is the density of the ReManed drives divided by \\(N\\), since they lose \\(1/N\\) of their capacity. Figure 4: The results from calculations and the simulations for the density of drives with one-head ReMan, i.e., \\(r_1(t)\\). We take \\(1/\\lambda_\\text{R}\\)=20 years, \\(\\kappa= 0.2\\) and \\(N=18\\). It is important to note that the failure rates in this example are taken unrealistically high to show the curvature in the lines. The markers show the Monte Carlo simulation results with one \\(\\sigma\\) error bars.  [+] We also take \\(\\beta_\\text{R}=1\\), and \\(\\beta_\\text{NR}=1.\\) \\(\\text{Total failure rate}(\\%)\\) \\(\\text{Percentage of Reman&#39;able Failure Rate}(\\%)\\) y:Lin/Log \\(\\text{CDF/Density/}\\Delta CAP\\) Time(yr) Swap Rate[no ReMan] Swap Rate [one-head ReMan] Ratio of ReManed drives Avg. CAP Loss[one-head ReMan] Figure 5: vs time. Note the switches to toggle the y-axis between linear vs log scale and between CDF/Density and capacity degredation.  [+] We also take \\(\\beta_\\text{R}=1\\), and \\(\\beta_\\text{NR}=1.\\) ReMan improves data durability Quick notes on erasure coding Hide The durability of data in storage systems is determined by physical device failure rates. A typical storage system has multiple storage devices, which significantly increases the likelihood of failures. In order to improve the reliability, data redundancy is added to the system so that it becomes possible to recover data on a failed storage device from the parities stored on the remaining healthy devices. For example, in the case of Redundant Array of Inexpensive Disks (RAID), adding one redundancy ensures data robustness to one failure, whereas adding two redundancies will provide robustness against two simultaneous failures[3]. The data durability can be further increased by implementing sophisticated erasure coding schemes[4][8] and also by prioritizing the recovery of critically damaged data stripes[9]. In all of these schemes, the durability critically depends on how fast the data on a failed device can be reconstructed to restore the redundancy. Consider the simplest case: one redundancy. Given a head/drive failure, there are two paths to losing data: A second failure while recovering from a head failure: less likely due to faster recovery. A second failure while recovering from a drive failure: more likely due to longer recovery. Figure 6: Markov chain split into two parallel paths: recovery from subcomponent failures and recovery from device failures. The plot on the left shows two parallel Markov chains: Left-most shows data recovery for a ReManable failure. Right-most shows the standard recovery when a drive fails. \\(r_m\\) term represents the maximally ReManed drive population. ReManable failures on maximal drives trigger replacement. This is represented by the dashed red curves. The transition rates are time dependent, and involve \\(r_m\\) functions. \\(\\mathcal{R}^{(1)}_1(t)=exp\\Big(-\\frac{\\mathcal{C}}{2 \\Gamma}\\left[t+\\frac{1-e^{-2 \\lambda_\\mathrm{R} t}}{2 \\lambda_\\mathrm{R}}\\right]-\\frac{\\mathcal{C}}{2\\mu}\\left[ (1+2 \\kappa)t-\\frac{1-e^{-2 \\lambda_\\mathrm{R} t}}{2\\lambda_\\mathrm{R}}\\right]\\Big)\\) where \\(\\mathcal{C}\\equiv n^2\\lambda^2_{\\mathrm{R}}(1+\\kappa)\\), and \\(\\kappa\\equiv\\frac{\\lambda_\\mathrm{NR}}{\\lambda_\\mathrm{R}}\\) \\(n:\\) number of drives in the EC scheme, \\(N:\\) number of heads per drive, \\(\\Gamma:\\) ReMan Repair rate, \\(\\mu:\\) drive repair rate, \\(\\lambda_R\\): ReManable failure rate, \\(\\lambda_{NR}\\): non-ReManable failure rate. This is the expression when we allow for 1 ReMan/drive. Similar formulas are calculated for \\(R_{max}=2,3\\). See [2] for the details. We have an analytical model of Erasure Coded systems that support ReMan. The closed mathematical form can be computed instantly enabling a real-time web application. Simulation with online ReMan Figure 7 shows the comparison of Monte Carlo Simulation and theoretical results[Assuming 1-ReMan/drive is allowed.]: ReManable AFR= 8% and non ReManable AFR=2%.[Taken unrealistically high to show the functional behavior] The plot shows the theoretical prediction is remarkably accurate: The gain in durability is about 4x. Figure 7: The calculated values of number of nines without ReMan(green curve), with ReMan(orange curve) and the simulation results for number of nines with ReMan (blue dots). Input parameters are taken as \\(\\lambda_\\mathrm{R}=1/12\\) and \\(\\lambda_\\mathrm{NR}=1/50\\) (1/year), which correspond to \\(8\\%\\) and \\(2\\%\\) AFR respectively. Modeling variable failure rates So far we have assumed that \\(\\lambda_\\mathrm{R}\\) and \\(\\lambda_\\mathrm{NR}\\) are constants, and in this chapter we will generalize the analysis to generic, time dependent \\(\\lambda\\). The main difficulty in modeling is that the differential equation will have time dependent coefficients, and it will be harder to solve analytically. However, it turns out that if we assume that the ratio of the ReManable failure rate and the non-Remanable one stays constant, we can still solve the problem analytically. Details on the computations Collapse We start from Eq. (16) , and rewrite it as follows: \\[\\begin{eqnarray} \\dot r_0(t)&amp;=&amp; -\\left[ \\frac{2N-1}{N} \\lambda_\\mathrm{R} +\\lambda_\\mathrm{NR} \\right]r_0 +\\lambda_\\mathrm{NR}+\\frac{N-1}{N} \\lambda_\\mathrm{R}\\equiv a(t) r_0(t)+b(t), \\tag{21} \\end{eqnarray}\\] with \\[\\begin{eqnarray} a(t)=-\\left[ \\frac{2N-1}{N} \\lambda_\\mathrm{R} +\\lambda_\\mathrm{NR} \\right], \\quad \\text{and}\\quad b(t)=\\lambda_\\mathrm{NR}+\\frac{N-1}{N} \\lambda_\\mathrm{R}. \\tag{22} \\end{eqnarray}\\] The solution to the ordinary differential equation in Eq. (21) is given by \\[\\begin{eqnarray} r_0(t)=e^{A(t)}\\left[1 + \\int_0^t d\\tau e^{-A(\\tau)} b(\\tau)\\right], \\tag{23} \\end{eqnarray}\\] where we defined \\(A(t)\\) as \\[\\begin{eqnarray} A(t)=\\int_0^t d\\tau \\,a(\\tau). \\tag{24} \\end{eqnarray}\\] Whether the integrals in Eq. (23) can be evaluated analytically will depend on the functional forms of \\(\\lambda_\\mathrm{R}\\) and \\(\\lambda_\\mathrm{NR}\\). Proportional failure rates Let us first consider a special case for which \\(\\lambda_\\mathrm{NR}=\\kappa\\lambda_\\mathrm{R}\\), i.e., the ReManable failure rate and the non-ReManable one have the same functional form. This will enable us to combine the terms and compute the integrals. To simplify the notation, we introduce a pair of new functions: \\[\\begin{eqnarray} \\lambda_\\mathrm{R}(t)\\equiv\\frac{d \\omega_\\mathrm{R}}{dt}, \\quad\\text{and}\\quad \\lambda_\\mathrm{NR}(t)\\equiv\\frac{d \\omega_{\\mathrm{NR}}}{dt}, \\tag{25} \\end{eqnarray}\\] and compute \\(A(t)\\) in Eq. (24) \\[\\begin{eqnarray} A(t)&amp;=&amp;-\\int_0^t d\\tau \\,\\frac{d}{d\\tau}\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R} +\\omega_{\\mathrm{NR}} \\right]=- \\frac{2N-1+N\\kappa}{N} \\omega_\\mathrm{R}. \\tag{26} \\end{eqnarray}\\] The integral in Eq. (23) can be evaluated as \\[\\begin{eqnarray} I&amp;\\equiv&amp;\\int_0^t d\\tau\\, e^{-A(\\tau)} b(\\tau)=\\frac{N-1+N \\kappa }{N} \\int_0^t d\\tau \\,e^{\\frac{2N-1+N\\kappa}{N} \\omega_\\mathrm{R}} \\frac{d \\omega_\\mathrm{R}}{d\\tau}\\nonumber\\\\ &amp;=&amp; \\left[ e^{- \\frac{2N-1+N\\kappa}{N} \\omega_\\mathrm{R}(t) }-1\\right] \\frac{N-1+N\\kappa}{2N-1+N\\kappa}. \\tag{27} \\end{eqnarray}\\] Putting it back in Eq. (23) yields \\[\\begin{eqnarray} r_0(t)=\\frac{N-1+N\\kappa+ N e^{- \\frac{2N-1+N\\kappa}{N} \\omega_\\mathrm{R}(t)}}{2N-1+N\\kappa}=1-\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\omega_\\mathrm{R} (t)}\\right), \\tag{28} \\end{eqnarray}\\] and using \\(r_0+r_1=1\\) we get: \\[\\begin{eqnarray} r_1(t)=\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\omega_\\mathrm{R} (t)}\\right). \\tag{29} \\end{eqnarray}\\] It may look surprising that Eq. (28) is very similar to Eq. (17) , the only difference being the change from \\(\\lambda_\\mathrm{R} t\\) to \\(\\omega_\\mathrm{R} (t)\\). However, one could see this coming: if \\(\\lambda_\\mathrm{NR}=\\kappa\\lambda_\\mathrm{R}\\), all the explicit time dependence can simply be absorbed by defining a time parameter \\(\\xi\\equiv \\,\\omega_\\mathrm{R} (t)\\). This will transform \\(\\frac{d}{dt}\\) to \\(\\frac{d\\xi}{dt}\\frac{d}{d\\xi}=\\frac{d\\omega_\\mathrm{R}}{dt}\\frac{d}{d\\xi}=\\lambda_\\mathrm{R}\\frac{d}{d\\xi}\\), and \\(\\lambda_\\mathrm{R}\\) factor on the left of the derivative operator cancels out with the same factor on the right hand side of Eq. (16) and hides away the variable rates into the warped time parameter \\(\\xi\\). Equation (29) describes how the density of ReManed drives will increase given any generic \\(w_\\mathrm{R}(t)\\). In the case of Weibull distribution, we will have \\[\\begin{eqnarray} \\omega_\\mathrm{R}(t)=\\left(\\frac{t}{\\alpha}\\right)^{\\beta}. \\tag{30} \\end{eqnarray}\\] Figure 8 shows various functions where the failure rates are time dependent but the ratio of the ReManable failure rate and the non-Remanable one stays constant, i.e., \\(\\lambda_\\mathrm{NR}=\\kappa\\lambda_\\mathrm{R}\\). The parameter \\(\\beta\\) sets the shape of the failure distribution, as in Eqs (1) , (2) , (3) , and Fig 1. Note that we will use a more intuitive parameter to replace \\(\\alpha\\), and call it Total Failure Rate in the first year, which is, as the name suggests, the total failure at the end of the first year, i.e., CDF at 1 year. Note that the failure rate is time dependent, and it will be a different number in the following years. In fact, it will get larger if \\(\\beta&gt;1\\). Once we define the first year failure rate and the shape parameter \\(\\beta\\), the failure distribution is completely fixed. The value of \\(\\alpha\\) can be expressed in Total Failure Rate in the first year by inverting Eq. (2) at \\(t=1\\)(year) as follows \\[\\begin{eqnarray} \\alpha=\\left(-ln[1-\\text{Total Failure Rate in the first year}]\\right)^{-1/\\beta}, \\tag{31} \\end{eqnarray}\\] which is the number that enters into the Weibull functions. \\(\\text{Total FR in the first year}(\\%)\\) \\(\\beta\\) \\(\\text{Ratio of Reman&#39;able FR}(\\%)\\) y:Lin/Log \\(\\text{Hazard/PDF/CDF/Density/}\\Delta CAP\\) Time(yr) Swap Rate[no ReMan] Swap Rate [one-head ReMan] Ratio of ReManed drives Avg. CAP Loss[one-head ReMan] Figure 8: vs time. Note the switches to toggle the y-axis between linear vs log scales.  [+] It is important to note that we assume the ratio of the ReManable failure rate and the non-Remanable one stays constant. Generic Failure Rates For completeness, let us consider the case where \\(\\lambda_\\mathrm{NR}\\) and \\(\\lambda_\\mathrm{R}\\) are not necessarily proportional to each other. They might be certain empirical distributions extracted from data. It really comes down to evaluating the integral Eq. (23) , and that can be done numerically. However, we can at least evaluate the dominant portion analytically, and refer to numerical methods for the rest. Details on the computations Collapse We start from Eq. (23) and split it into two parts as follows: \\[\\begin{eqnarray} I&amp;=&amp;\\int_0^t d\\tau\\, e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(\\tau) +\\omega_{\\mathrm{NR}}(\\tau) \\right]}\\frac{d}{d\\tau}\\left[\\omega_{\\mathrm{NR}}+\\frac{N-1}{N} \\omega_\\mathrm{R}\\right]\\nonumber\\\\ &amp;=&amp; \\frac{N-1}{2N -1} \\int_0^t d\\tau\\, e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(\\tau) +\\omega_{\\mathrm{NR}}(\\tau) \\right]}\\frac{d}{d\\tau}\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}+ \\omega_{\\mathrm{NR}}+\\frac{N}{N-1}\\omega_{\\mathrm{NR}} \\right]\\nonumber\\\\ &amp;=&amp; \\frac{N-1}{2N -1}\\left(e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(t) +\\omega_{\\mathrm{NR}}(t) \\right]} -1\\right)+ \\frac{N}{2N-1}\\int_0^t d\\tau\\, e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(\\tau) +\\omega_{\\mathrm{NR}}(\\tau) \\right]}\\frac{d\\omega_{\\mathrm{NR}}}{d\\tau}. \\tag{32} \\end{eqnarray}\\] It may look like we have not accomplished much since we still have an integral left, however, in practice \\(\\lambda_\\mathrm{NR}\\) will give a smaller contribution to the overall failures [it can be as low as \\(20\\%\\) of the total failure rate.] We can still evaluate the remaining integral numerically. Alternatively, we can approximate the integral as \\(\\omega_{\\mathrm{NR}}(t)\\ll 1\\) and \\(\\omega_{\\mathrm{NR}}(t)\\ll 1\\): \\[\\begin{eqnarray} I_2&amp;\\equiv&amp;\\frac{N}{2N-1}\\int_0^t d\\tau\\, e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(\\tau) +\\omega_{\\mathrm{NR}}(\\tau) \\right]}\\frac{d\\omega_{\\mathrm{NR}}}{d\\tau}\\simeq \\frac{N}{2N-1}\\int_0^t d\\tau\\, \\left[1+ \\frac{2N-1}{N} \\omega_\\mathrm{R}(\\tau) +\\omega_{\\mathrm{NR}}(\\tau) \\right]\\frac{d\\omega_{\\mathrm{NR}}}{d\\tau}\\nonumber\\\\ &amp;\\simeq&amp;\\frac{N}{2N-1} \\omega_{\\mathrm{NR}}(t) + \\frac{N}{2(2N-1)} \\omega^2_\\mathrm{NR}(t)+\\frac{N}{2N-1}\\int_0^t d\\tau \\,\\omega_\\mathrm{R}(\\tau)\\frac{d\\omega_{\\mathrm{NR}}}{d\\tau}. \\tag{33} \\end{eqnarray}\\] If we use Weibull expressions for \\(\\omega\\), as given in (30) : \\[\\begin{eqnarray} \\int_0^t d\\tau \\,\\omega_{\\mathrm{R}}(\\tau)\\frac{d\\omega_{\\mathrm{NR}}}{d\\tau}&amp;=&amp;\\frac{\\beta_{\\text{NR}}}{\\alpha_{\\text{NR}}}\\int_0^t d\\tau \\left(\\frac{t}{\\alpha_{\\text{R}}}\\right)^{\\beta_\\text{R}} \\left(\\frac{t}{\\alpha_{\\text{NR}}}\\right)^{\\beta_{\\text{NR}-1}}=\\frac{\\beta_{\\text{NR}}}{(\\alpha_{\\text{NR}})^{\\beta_{\\text{NR}}}(\\alpha_{\\text{R}})^{\\beta_{\\text{R}}}} \\int_0^t\\, d\\tau\\, \\tau^{\\beta_\\text{R}+\\beta_{\\text{NR}}-1}\\nonumber\\\\ &amp;=&amp;\\frac{\\tau^{\\beta_\\text{R}+\\beta_{\\text{NR}}}}{(\\alpha_{\\text{NR}})^{\\beta_{\\text{NR}}}(\\alpha_{\\text{R}})^{\\beta_{\\text{R}}}} =\\frac{\\beta_{\\text{NR}}}{\\beta_\\text{R}+\\beta_{\\text{NR}}} \\omega_{\\mathrm{R}}(t)\\omega_{\\mathrm{NR}}(t). \\tag{34} \\end{eqnarray}\\] Compiling the terms yields: \\[\\begin{eqnarray} I&amp;\\simeq&amp; \\frac{N-1}{2N -1}\\left(e^{\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(t) +\\omega_{\\mathrm{NR}}(t) \\right]} -1\\right) +\\frac{N}{2N-1} \\omega_{\\mathrm{NR}}(t) + \\frac{N}{2(2N-1)} \\omega^2_\\mathrm{NR}(t) +\\frac{N}{2N-1}\\frac{\\beta_{\\text{NR}}}{\\beta_\\text{R}+\\beta_{\\text{NR}}} \\omega_{\\mathrm{R}}(t)\\omega_{\\mathrm{NR}}(t) \\tag{35} \\end{eqnarray}\\] Finally we put the results back in Eq. (23) and get \\[\\begin{eqnarray} r_0(t)&amp;\\simeq&amp; \\frac{N-1}{2N -1} +\\frac{N}{2N -1}e^{-\\left[ \\frac{2N-1}{N} \\omega_\\mathrm{R}(t) +\\omega_{\\mathrm{NR}}(t) \\right]} \\left[1+ \\omega_{\\mathrm{NR}}(t) + \\frac{1}{2} \\omega^2_\\mathrm{NR}(t) +\\frac{\\beta_{\\text{NR}}}{\\beta_\\text{R}+\\beta_{\\text{NR}}} \\omega_{\\mathrm{R}}(t)\\omega_{\\mathrm{NR}}(t)\\right]. \\tag{36} \\end{eqnarray}\\] It is important to note that this equation holds when failure modes are defined by Weibull distributions. For other distributions, one needs to evaluate the integral in Eq. (32) . References [1] W. J. Padgett, Weibull distribution, in International encyclopedia of statistical science, M. Lovric, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 16511653 [Online]. Available: https://doi.org/10.1007/978-3-642-04898-2_611 [2] S. Ölmez, Reliability analysis of storage systems with partially repairable devices, IEEE Transactions on Device and Materials Reliability, pp. 11, 2021, doi: 10.1109/TDMR.2021.3077848. [Online]. Available: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9424170 [3] D. A. Patterson, G. Gibson, and R. H. Katz, A case for redundant arrays of inexpensive disks (RAID), SIGMOD Rec., vol. 17, no. 3, pp. 109116, Jun. 1988, doi: 10.1145/971701.50214. [Online]. Available: https://doi.org/10.1145/971701.50214 [4] L. Hellerstein, G. A. Gibson, R. M. Karp, R. H. Katz, and D. A. Patterson, Coding techniques for handling failures in large disk arrays, Algorithmica, vol. 12, no. 2, pp. 182208, Sep. 1994, doi: 10.1007/BF01185210. [Online]. Available: https://doi.org/10.1007/BF01185210 [5] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. J. Wainwright, and K. Ramchandran, Network coding for distributed storage systems, IEEE Trans. Inf. Theor., vol. 56, no. 9, pp. 45394551, Sep. 2010, doi: 10.1109/TIT.2010.2054295. [Online]. Available: https://doi.org/10.1109/TIT.2010.2054295 [6] G. Wang, L. Xiao-Guang, and L. Jing, Parity declustering data layout for tolerating dependent disk failures in network RAID systems, 2002, pp. 2225, doi: 10.1109/ICAPP.2002.1173547. [7] V. Venkatesan and I. Iliadis, Effect of codeword placement on the reliability of erasure coded data storage systems, in Quantitative evaluation of systems, 2013, pp. 241257. [8] A. Thomasian and M. Blaum, Higher reliability redundant disk arrays: Organization, operation, and coding, ACM Trans. Storage, vol. 5, no. 3, Nov. 2009, doi: 10.1145/1629075.1629076. [Online]. Available: https://doi.org/10.1145/1629075.1629076 [9] T. Kawaguchi, Reliability analysis of distributed RAID with priority rebuilding, 2013. "]]
