[["ReMan_model.html", "Modeling Drive Reliability with ReMan ReMan provides substantial benefits Modeling failure distributions Visualizing failure distributions Modeling One-Head ReMan Modeling k-Head ReMan ReMan eliminates head related failures ReMan preserves capacity at the edge ReMan reduces swap rate and TCO in data centers ReMan improves data durability Simulation with online ReMan References", " Modeling Drive Reliability with ReMan Serkay Ã–lmez April 26, 2021 Abstract The goal of this work is to develop a complete model to study the benefits of depopulating heads in the field. These benefits include the extension of the device lifetime and the gain in the data durability for erasure coded systems. We will assume that there will be two failure modes: the first one is head related, causing individual heads to fail, and the other will be at the device level, failing the whole device. The model will be flexible to address various cases. The model is supported by simulations. .sliderlabel{ text-align:center; font-size:12px; margin-top: 0.7em; } ReMan provides substantial benefits The latest generation of HDDs has up to 20 heads: A significant portion of failures is head specific. The ReMan technology enables drives to continue to run even after a certain number of head failures. This practically eliminates head specific failures and reduces the overall failure rate by up to a factor of \\(5\\)X. Online ReMan enables substantial gains in data durability. Online ReMan enables much faster data recovery. For a 20-header, 20TB drive, only 1TB needs to be recovered rather than 20TB. The latest generation of HDDs has up to 20 heads. A significant portion of failures is head specific. The ReMan [ReManufacturing drives with head failures] technology enables drives to continue to run even after a certain number of head failures. This practically eliminates head specific failures and reduces the overall failure rate by up to a factor of \\(5\\)X.Online ReMan enables much faster data recovery. For a 20-header, 20TB drive, only 1TB needs to be recovered rather than 20TB. serkay.olmez@seagate.com Acknowledgments: Iman Anvari, Mike Barrell, John Bent, Drew Cheney, Ian Davies, Cary Johnson, Erin Foley and Matt Shumway Set the level of detail with the buttons above Press F11 for full screen Ctrl +/- to fit the content into the browser Use arrow keys to navigate the slides Table of contents button on the left bottom Printable PDF: This is a static copy of the presentation. Compiled with BookDown xie2015? and Revealjs revealjs? Fragments: Enable or disable fragment transitions within slides .mathjax-tooltip { display: none; width: 100%; position: absolute; background-color:gray; } Modeling failure distributions Weibull distribution [1] is a versatile function due to its flexibility to fit distributions of various shapes. We first consider a particular device failure mode, and assume it can be accurately fit by a Weibull distribution. A Weibull distribution has two parameters: \\(\\alpha\\) parameter sets the time scale whereas the shape parameter \\(\\beta\\) sets the shape of the distribution. The failure probability density, cumulative failure distributions and the hazard rate (failure rate) are defined as follows: \\[\\begin{equation} f_{\\alpha,\\beta}(t)=\\frac{\\beta }{\\alpha} \\left( \\frac{t}{\\alpha}\\right)^{\\beta -1} e^{-\\left( \\frac{t}{\\alpha}\\right)^{\\beta}},\\tag{1} \\end{equation}\\] \\[\\begin{equation} F_{\\alpha,\\beta}(t)= \\int_{0}^{t} d\\tau f_{\\alpha,\\beta}(\\tau)=1-e^{-\\left(\\frac{t}{\\alpha}\\right)^{\\beta}},\\tag{2} \\end{equation}\\] \\[\\begin{equation} h_{\\alpha,\\beta}(t)= \\frac{f_{\\alpha,\\beta}(t)}{1-F_{\\alpha,\\beta}(t)}=\\frac{\\beta }{\\alpha} \\left( \\frac{t}{\\alpha}\\right)^{\\beta -1}.\\tag{3} \\end{equation}\\] The hazard rate function, \\(h_{\\alpha,\\beta}(t)\\), is the event rate normalized with respect the to the population that survived until \\(t\\). The actual values of \\(\\alpha\\) and \\(\\beta\\) vary from product to product: \\(\\beta\\) is expected to be between \\(1\\) and \\(2\\). When \\(\\beta\\neq1\\), the failure rates are time dependent. It is certainly possible to model ReMan with \\(\\beta\\neq1\\), but \\(\\beta=1\\) is typically a good fit, and that is what we will assume in this work. Visualizing failure distributions In the interactive plot below, \\(\\alpha\\) and \\(\\beta\\) values can be adjusted. The toggles on the right change the scales. \\(\\beta\\) \\(\\alpha\\) x:lin/log y:lin/log Figure 1: An interactive plot of Weibull distributions defined in Eqs. (1) , (2) and (3). Use the toggles for linear/log scales.  [+] The vertical dashed lines mark the value \\(t=\\alpha\\), and the horizontal one on \\(F\\) marks the value \\(0.63\\). The \\(y\\) axis can be transformed to \\(ln\\left[-ln(1-F_{\\alpha,\\beta})\\right]\\), and the time axis can be transformed to \\(ln(t)\\). The effect of \\(\\beta\\) on \\(F\\) is best observed with the log time scale, and \\(y\\) in the probability scale: notice how \\(F_{\\alpha,\\beta}\\) becomes a line with the slope \\(\\beta\\). Selecting the log scale for time and linear scale for the \\(y\\) axis, shows that the changing scale parameter \\(\\alpha\\) moves the functions horizontally while preserving their shapes. The actual values of \\(\\alpha\\) and \\(\\beta\\) vary from product to product: \\(\\beta\\) is expected to be between \\(1\\) and \\(2\\). When \\(\\beta\\neq1\\), the failure rates are time dependent. It is certainly possible to model ReMan with \\(\\beta\\neq1\\), but \\(\\beta=1\\) is typically a good fit, and that is what we will assume in this work. \\(\\beta=1\\) gives the exponential distribution, which has completely random head failure times with a fixed failure(hazard) rate: \\(h_{\\alpha,\\beta}(t)=\\frac{1}{\\alpha}\\equiv\\lambda\\). This simplifies Eqs. (1) , (2) and (3) to: \\[\\begin{equation} f_{\\lambda}(t)= \\lambda e^{-\\lambda t},\\quad F_{\\lambda}(t)=1-e^{-\\lambda t},\\, \\rm{and}\\quad h_{\\lambda}(t)=\\lambda. \\tag{4} \\end{equation}\\] It is important to understand how the storage devices fail. Weibull is typically a good fit. No matter how reliable individual devices are, failures are inevitable. Therefore, redundancies are needed to preserve data in case of drive failures. Modeling One-Head ReMan The probability of having no failures in an \\(N\\)-header drive is: \\[\\begin{equation} p_{head}^{ all\\, pass}=\\left(p_{head}^{pass}\\right)^N=e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}, \\tag{5} \\end{equation}\\] where \\(\\alpha\\) and \\(\\beta\\) are head-level Weibull parameters. The probability of having exactly one head failure is \\[\\begin{eqnarray} p_{drive}^{ exactly\\,one\\, head \\,fail }&amp;=&amp;N\\,\\left(p_{head}^{fail}\\right)\\left(p_{head}^{pass}\\right)^{N-1}=N\\,\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right) e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{6} \\end{eqnarray}\\] Then probability of having two or more failures in an \\(N\\)-header drive (\\(CFR_R\\)) reads: \\[\\begin{equation} %1-e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}-N\\,\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right) e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}} CFR_R=N \\left(1-e^{-(N-1)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)-(N-1)\\left(1-e^{-N\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right). \\tag{7} \\end{equation}\\] Eq. (7) is a mixture of two Weibull modes. The first term describes \\(N-1\\) header drives, and the second describes \\(N\\) header drives. This equation still uses the time per head, and we need to convert that to drive level time. After the first head failure, the head time will increase as \\(t_d/(N-1)\\). Scaling the time in Eq. (7), we get \\[\\begin{eqnarray} CFR_R^{drive}(t_d)&amp;=&amp;N \\left(1-e^{-(N-1)\\,\\left( \\frac{t_d}{\\alpha(N-1)}\\right)^{\\beta}}\\right) -(N-1)\\left(1-e^{-N\\,\\left( \\frac{t_d}{\\alpha(N-1)}\\right)^{\\beta}}\\right). \\tag{8} \\end{eqnarray}\\] Modeling k-Head ReMan Consider the case where at most \\(k\\)-head failure is acceptable. The probability of having up to \\(k\\) (including \\(k\\)) failures is \\[\\begin{eqnarray} p_{drive}^{ up\\, to\\, k \\, head \\,failures }&amp;=&amp;\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(p_{head}^{fail}\\right)^j\\left(p_{head}^{pass}\\right)^{N-j} =\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)^j e^{-(N-j)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{9} \\end{eqnarray}\\] Then the CFR for having more than \\(k\\) head failures becomes \\[\\begin{eqnarray} CFR(\\mathrm{k-ReMAN})&amp;=&amp;p_{drive}^{more\\, than\\, k \\, head\\,failures} =1-p_{drive}^{ up\\, to\\, k \\, head \\,failures }\\nonumber\\\\ &amp;=&amp;1-\\sum_{j=0}^{k}\\, \\binom{N}{j}\\left(1-e^{-\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}\\right)^j e^{-(N-j)\\,\\left( \\frac{t}{\\alpha}\\right)^{\\beta}}. \\tag{10} \\end{eqnarray}\\] ReMan eliminates head related failures There are two types of failures: Head specific failures, which can be handled by ReMan: Just depop the failing head, and move on. We will show that this practically eliminates head related failures. Entire device failures: motor/PCBA/HSA failures, etc. These cannot be addressed by ReMan. Data suggests that a significant portion of the failures fall into the ReManable bucket. The overall reliability of a device can be computed by the multiplying the individual reliability functions, \\(R_i\\): \\[\\begin{equation} R(t)={\\displaystyle \\prod_{i}}R_i= R_\\text{ReMan&#39;able}\\times R_\\text{non-ReMan&#39;able}. \\tag{11} \\end{equation}\\] \\(R_\\text{non-ReMan&#39;able}\\) is the reliability associated with the entire device failures, and it can simply be defined by a failure rate \\(\\lambda_\\text{NR}\\). \\(R_\\text{ReMan&#39;able}\\), on the other hand, is the associated with the head-specific failures. They are fixable by depoping heads. See Fig. 2 for an example simulation. See the caption for the parameters. See Fig. ?? for an example simulation. See the caption for the parameters. Show the Reman simulation script (R) Hide library(dplyr);library(plotly);library(rstudioapi); Nh=18; # number of heads per drive ndr=80000; #number of drives to simulate ### Reman&#39;able Failure mode parameters beta=1; # alpha=2500000 afr=0.8 # alpha&lt;-round(8760/(-log(1-afr/100))^(1/beta)) digits&lt;-floor(log10(alpha)) ### Non-Reman&#39;able Failure mode parameters betaNR=1; # alphaNR=2500000 afrNR=0.2 # in pct alphaNR&lt;-round(8760/(-log(1-afrNR/100))^(1/betaNR)) head_failure_time&lt;-rweibull(Nh*ndr, shape=beta, scale=alpha) # pull Nh* ndr weibull numbers hdid&lt;-c(0:(Nh*ndr-1)); # create unique head id drvid&lt;-floor(hdid/Nh) # group heads into drives fdf&lt;-data.frame(&quot;drvid&quot;=drvid,&quot;hdid&quot;=hdid,&quot;head_failure_time&quot;=head_failure_time)# putting the columns into a dataframe fdf&lt;-fdf[ with(fdf, order(drvid,head_failure_time)), ] mindf&lt;-fdf%&gt;% group_by(drvid) %&gt;% summarise (hft1= min(head_failure_time))# locating the first head failure mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindf, by=&quot;drvid&quot;, all.x = TRUE) # joining it back into the data frame. fdfs&lt;-fdf[!fdf$hft1==fdf$head_failure_time,] mindfs&lt;-fdfs%&gt;% group_by(drvid) %&gt;% summarise (hft2= min(head_failure_time)) mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindfs, by=&quot;drvid&quot;, all.x = TRUE) fdf$drv_ft_no_Reman&lt;-Nh*fdf$hft1;fdf$drv_ft_one_Reman&lt;-(Nh-1)*fdf$hft2+fdf$hft1 fdfs&lt;-fdf[fdf$head_failure_time&gt;fdf$hft2,] mindfs&lt;-fdfs%&gt;% group_by(drvid) %&gt;% summarise (hft3= min(head_failure_time)) mindf&lt;-as.data.frame(mindf);fdf&lt;-merge(x = fdf, y = mindfs, by=&quot;drvid&quot;, all.x = TRUE) fdf$drv_ft_two_Reman&lt;-(Nh-2)*fdf$hft3+fdf$hft2+fdf$hft1 dfsum&lt;-fdf%&gt;% group_by(drvid)%&gt;% summarise (drive_ft_no_Reman= min(drv_ft_no_Reman),drive_ft_one_Reman= min(drv_ft_one_Reman),drive_ft_two_Reman= min(drv_ft_two_Reman)) dfsum&lt;-as.data.frame(dfsum) dfsum$driveNR_time&lt;-rweibull(ndr, shape=betaNR, scale=alphaNR) # pull Nh* ndr weibull numbers dfsum &lt;- transform(dfsum, drv_failure_time_one_Reman = pmin(drive_ft_one_Reman, driveNR_time)) dfsum &lt;- transform(dfsum, drv_failure_time_two_Reman = pmin(drive_ft_two_Reman, driveNR_time)) dfsum &lt;- transform(dfsum, drv_failure_time_no_Reman = pmin(drive_ft_no_Reman, driveNR_time)) stepsize&lt;-min(round(alpha/2000),round(alphaNR/2000)) dfsum$time&lt;-stepsize*round(dfsum$drv_failure_time_one_Reman/stepsize,0) dfsums&lt;-dfsum%&gt;%group_by(time) %&gt;% summarise(count=n()) dfsums&lt;-as.data.frame(dfsums);dfsums$density&lt;-dfsums$count/ndr dfsums &lt;- within(dfsums, cdf_one_Reman &lt;- cumsum(density)) dfsums$cdf_one_Reman&lt;- 100*dfsums$cdf_one_Reman dfsum$timez&lt;-stepsize*round(dfsum$drv_failure_time_no_Reman/stepsize,0) dfsumsz&lt;-dfsum%&gt;% group_by(timez) %&gt;% summarise(countz=n()) dfsumsz&lt;-as.data.frame(dfsumsz);dfsumsz$densityz&lt;-dfsumsz$countz/ndr dfsumsz &lt;- within(dfsumsz, cdf_no_Reman &lt;- cumsum(densityz)) dfsumsz$cdf_no_Reman&lt;- 100*dfsumsz$cdf_no_Reman dfsums&lt;-merge(x = dfsums, y = dfsumsz[,c(&quot;timez&quot;,&quot;cdf_no_Reman&quot;)], by.x=&quot;time&quot;, by.y=&quot;timez&quot;,all.x = TRUE) dfsum$timez&lt;-stepsize*round(dfsum$drv_failure_time_two_Reman/stepsize,0) dfsumsz&lt;-dfsum%&gt;% group_by(timez) %&gt;% summarise(countz=n()) dfsumsz&lt;-as.data.frame(dfsumsz);dfsumsz$densityz&lt;-dfsumsz$countz/ndr dfsumsz &lt;- within(dfsumsz, cdf_two_Reman &lt;- cumsum(densityz)) dfsumsz$cdf_two_Reman&lt;- 100*dfsumsz$cdf_two_Reman dfsums&lt;-merge(x = dfsums, y = dfsumsz[,c(&quot;timez&quot;,&quot;cdf_two_Reman&quot;)], by.x=&quot;time&quot;, by.y=&quot;timez&quot;,all.x = TRUE) dfsums$cdf_no_Reman_Theory&lt;-100*(1-exp(-(Nh)*((dfsums$time/alpha)/(Nh))^beta-(dfsums$time/alphaNR)^beta)) dfsums$cdf_one_Reman_Theory&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta) * (exp(-Nh*((dfsums$time/alpha)/(Nh))^beta) +Nh*(1-exp(-((dfsums$time/alpha)/(Nh))^beta)) *exp(-(Nh-1)*((dfsums$time/alpha)/(Nh))^beta))) dfsums$cdf_two_Reman_Theory&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta)* (exp(-Nh*((dfsums$time/alpha)/(Nh))^beta) +Nh*(1-exp(-((dfsums$time/alpha)/(Nh))^beta))*exp(-(Nh-1) *((dfsums$time/alpha)/(Nh))^beta) +Nh*(Nh-1)/2*(1-exp(-((dfsums$time/alpha)/(Nh))^beta))^2 *exp(-(Nh-2)*((dfsums$time/alpha)/(Nh))^beta)) ) dfsums$cdf_Ratio_Theory&lt;-dfsums$cdf_no_Reman_Theory/dfsums$cdf_one_Reman_Theory dfsums$dtimey&lt;-dfsums$time*Nh dfsums$time_year&lt;-dfsums$time/8760 dfsums$beta&lt;-beta;dfsums$betaNR&lt;-betaNR dfsums$alpha&lt;-alpha;dfsums$alphaNR&lt;-alphaNR dfsums$afr&lt;-afr;dfsums$afrNR&lt;-afrNR dfsums$Nh&lt;-Nh;# number of heads per drive dfsums$Ndr&lt;-ndr dfsums$floor&lt;-100*(1-exp(-(dfsums$time/alphaNR)^beta)) #$sourcecode[1]&lt;-rstudioapi::getActiveDocumentContext()$path dfsums$creationdate[1]&lt;-Sys.time() write.csv(dfsums, file = &quot;C:/Users/451516/my_codes/R_simulation_codes/Remanable_and_nonReManable_failures.csv&quot;) t &lt;- list( size = 14) ay &lt;- list( tickfont = list(color = &quot;black&quot;), overlaying = &quot;y&quot;,side = &quot;right&quot;,title = &quot;CFR_gain&quot;) plot_ly(dfsums, x = ~time_year, y = ~cdf_no_Reman, name = &#39;CDF_no_Reman_Simulation&#39;, type = &#39;scatter&#39;, mode = &#39;markers+lines&#39;) %&gt;% add_trace(y = ~cdf_no_Reman_Theory, name = &#39;CDF_no_Reman_Theory&#39;, mode = &#39;lines&#39;) %&gt;% add_trace(y = ~cdf_one_Reman, name = &#39;CDF_one_Reman_Simulation&#39;, mode = &#39;markers+lines&#39;) %&gt;% add_trace(y = ~cdf_one_Reman_Theory, name = &#39;CDF_one_Reman_Theory&#39;, mode = &#39;lines&#39;) %&gt;% #add_trace(y = ~cdf_two_Reman, name = &#39;CDF_two_Reman_Simulation&#39;, mode = &#39;markers+lines&#39;) %&gt;% #add_trace(y = ~cdf_two_Reman_Theory, name = &#39;CDF_two_Reman_Theory&#39;, mode = &#39;markers+lines&#39;) %&gt;% #add_trace(y = ~floor, name = &#39;CDF_Non-Remanable&#39;, mode = &#39;lines&#39;, line=list(dash=&quot;dash&quot;)) %&gt;% layout(legend = list(x = 0.2, y = 0.95))%&gt;% layout( yaxis2 = ay, xaxis = list(title=&quot;Drive time (years)&quot;, range = c(0,5)), yaxis = list(title=&quot;Cumulative Failures(%)&quot;,range = c(-0.1,5.5)) )%&gt;% layout(margin=list(r = 80,l = 80,t=40,b = 40), yaxis=list(tickprefix=&quot; &quot;))%&gt;% layout(title=&quot;Cumulative Failures with and without Reman&quot;, font=t) #%&gt;%add_lines(y=~cdf_Ratio_Theory, name = &quot;CFR_gain&quot;, yaxis = &quot;y2&quot;) Figure 2: Comparison of cumulative failures with and without ReMan. Total failure rate is taken as \\(1\\%\\) split into \\(0.8\\%\\) ReManable and \\(0.2\\%\\) Non-ReManable.  [+] We also take \\(\\beta_{R}=1\\), and \\(\\beta_{NR}=1.\\) Click on the legend to display the hidden traces. Even one-head ReMan absorbs practically all head specific failures. In this example, the Annualized Failure Rate (AFR) is effectively reduced down to \\(0.2\\%\\) from the original value of \\(1\\%\\). See Fig.5 for an interactive plot to adjust the total AFR and its split into the two categories. ReMan preserves capacity at the edge Details on the capacity computations Collapse Consider an edge application in which failed drives are not replaced immediately. ReMan will be an indispensable tool to preserve the capacity of the storage system: Entire device failures will take down the full drive causing the loss of the full capacity. Head specific failures will be ReManed, losing \\(1/N\\) of the full cap (\\(5\\%\\) for a 20-header drive). ReMan helps preserve capacity much longer. When no ReMan is allowed the average capacity will drop with drive failures, which can be written as follows: \\[\\begin{equation} \\mathcal{CAP}_{0R}=\\mathcal{CAP}_0(1-CFR_{0R}) \\tag{12} \\end{equation}\\] When at most one ReMan is allowed, the average capacity will be \\[\\begin{equation} \\mathcal{CAP}_{1R}=\\mathcal{CAP}_0(1-CFR_{1R}) -\\mathcal{CAP}_0(CFR_{0R}-CFR_{1R})/N, \\tag{13} \\end{equation}\\] where the second term is the ratio of the drives with exactly one head failure [they lost a capacity of \\(\\mathcal{CAP}_0/N\\)]. Figure 3 shows the CFR and capacity. \\(\\text{First Year Failure Rate(%), Left: Non-Reman&#39;able, Right:Total }\\) \\(\\text{Percentage of Reman&#39;able Failure Rate}(\\%)\\) \\(\\text{Total Failure Rate}(\\%)\\) \\(\\beta_\\text{NR}\\) \\(\\beta_\\text{R}\\) \\(\\alpha_\\text{NR}\\) (hours) \\(N\\) (number of heads) \\(WL\\) (TB/year written) \\(DR\\) (Avg. data rate in MB/s) y:lin/log CFR/CAP Target/Now Time(yr) CFR no ReMan CFR one-head ReMan Avg. CAP no ReMan Avg. CAP one-head ReMan Figure 3: vs time with Number of heads \\(N=20\\). Note the switches to toggle the y-axis between linear vs log scale and between CFR vs Capacity.  [+] We also take \\(\\beta_{R}=1\\), and \\(\\beta_{NR}=1.\\) ReMan reduces swap rate and TCO in data centers As shown earlier, ReMan substantially reduces drive failures and, in turn, the cost of replacing them. The only drawback of ReMan is that ReManed drives will have one-less heads, hence slightly reduced capacity. In the case of maximum one-head ReMan, the overall capacity of the system will reduce only slightly. Qualitatively speaking, the ratio of ReManed drives will increase slowly, because the increase is suppressed by two factors: As drives fail for the NonReManable mode, they are replaced. This removes already ReManed drives out of the population. If a second head in an already-ReManed drive fails, it is replaced. This also removes ReManed drives out of the population. The precise mathematical model is not trivial[2] due to the factors above, see the details below. The final expression for the ReManed drive density is given in Eq. (16) and it is plotted and compared with simulation results in Figure 4 for illustation purposes[see the caption for details]. Once the ReManed drive density is calculated, the degradation in the overall capacity can be computed as in Eq. (18) . How average capacity will change with time is shown in Fig. 5. See Fig. 5 to adjust the inputs to the model to observe how the average capacity changes. Also note that the time entry in the table below the plot is editable. Details on the computations Collapse When a head fails, it is turned off, and the drive continues to operate with reduced capacity. We will denote the ratio of devices that lost \\(i\\) head as \\(r_i\\), and compute how these ratios evolve with time. We first consider a replacement policy which allows for maximum one-head ReMan, which implies that \\(r_0(t)+r_1(t)=1\\). Furthermore, since the system starts with new devices, the initial values of the ratios are \\(r_0(0)=1\\) and \\(r_1(0)=0\\). We need to calculate how \\(r_0\\) evolves with time given that devices have two distinct failure modes: i) a failure mode that affects individual heads, ii) another failure mode that affects the whole device and results in device replacement. We define the failure rate that affects heads as \\(\\lambda_\\mathrm{R}\\), where R stands for Re-manufacturable, and the other mode as \\(\\lambda_\\mathrm{NR}\\), where NR stands for NonRe-manufacturable. As heads fail, the ratio of the devices with no head failures (\\(r_0\\)) will decrease: \\(\\dot r_0\\propto -\\lambda_\\mathrm{R}\\, r_0\\). On the other hand, devices that already have one head failure (\\(r_1\\)) will be replaced with new devices once another head fails. The replacement will move these devices to the \\(r_0\\) bucket. The rate of this event is proportional to the total number of heads in \\(r_1\\) bucket. Adding this term yields \\(\\dot r_0\\propto -\\lambda_\\mathrm{R}\\, r_0+ \\lambda_\\mathrm{R}\\,\\frac{N-1}{N}\\, r_1\\), where the factor \\(\\frac{N-1}{N}\\) normalizes the failure rate since there are \\(N-1\\) heads per device in the \\(r_1\\) bucket. There is one more term to add: the device level failures with rate \\(\\lambda_\\mathrm{NR}\\). Such failures require device replacements, increasing \\(r_0\\) only if the failure happens to be in the \\(r_1\\) bucket. Therefore the rate of such events will be \\(r_1 \\lambda_\\mathrm{NR}\\). Adding all the terms, and using \\(r_1=1-r_0\\) result in the following differential equation: \\[\\begin{eqnarray} \\dot r_0&amp;=&amp; -\\lambda_\\mathrm{R}\\, r_0+\\lambda_\\mathrm{R}\\,\\frac{N-1}{N}(1-r_0)+ \\lambda_\\mathrm{NR} (1-r_0). \\tag{14} \\end{eqnarray}\\] Eq. (14) has the following solution: \\[\\begin{equation} r_0(t)= \\frac{N-1+ N\\kappa+N e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}}{2N-1+N\\kappa }=1-\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}\\right), \\tag{15} \\end{equation}\\] where we defined \\(\\kappa=\\frac{\\lambda_\\mathrm{NR}}{\\lambda_\\mathrm{R}}\\). Since \\(r_0+r_1=1\\), we get \\[\\begin{equation} r_1(t)=1-r_0(t)=\\frac{1}{2+\\kappa-1/N }\\left(1- e^{-(2+\\kappa-1/N )\\lambda_\\mathrm{R} t}\\right), \\tag{16} \\end{equation}\\] which is the relative ratio of the ReManed drives. We can now write the average capacity of the system: \\[\\begin{equation} \\overline{CAP}=C(r_0+r_1\\frac{N-1}{N})=C(r_0+r_1-\\frac{r_1}{N})=C(1-\\frac{r_1}{N}) \\tag{17} \\end{equation}\\] Equivalently, we can define the relative the loss in the capacity: \\[\\begin{equation} \\frac{\\Delta\\overline{CAP}}{C}=\\frac{C-\\overline{CAP}}{C}=\\frac{r_1}{N}, \\tag{18} \\end{equation}\\] which basically is the density of the ReManed drives divided by \\(N\\), since they lose \\(1/N\\) of their capacity. Figure 4: The results from calculations and the simulations for the density of drives with one-head ReMan, i.e., \\(r_1(t)\\). We take \\(1/\\lambda_\\text{R}\\)=20 years, \\(\\kappa= 0.2\\) and \\(N=18\\). It is important to note that the failure rates in this example are taken unrealistically high to show the curvature in the lines. The markers show the Monte Carlo simulation results with one \\(\\sigma\\) error bars.  [+] We also take \\(\\beta_\\text{R}=1\\), and \\(\\beta_\\text{NR}=1.\\) \\(\\text{Percentage of Reman&#39;able Failure Rate}(\\%)\\) \\(\\text{Total failure rate}(\\%)\\) \\(\\beta_\\text{NR}\\) \\(\\beta_\\text{R}\\) \\(N\\) (number of heads) y:log/lin \\(\\text{CFR/Density/}\\Delta CAP\\) Time(yr) Swap Rate[no ReMan] Swap Rate [one-head ReMan] Ratio of ReManed drives Avg. CAP Loss[one-head ReMan] Figure 5: vs time. Note the switches to toggle the y-axis between linear vs log scale and between CFR/Density and capacity degredation.  [+] We also take \\(\\beta_{R}=1\\), and \\(\\beta_{NR}=1.\\) ReMan improves data durability A quick survey on literature Hide The durability of data in storage systems is determined by physical device failure rates. A typical storage system has multiple storage devices, which significantly increases the likelihood of failures. In order to improve the reliability, data redundancy is added to the system so that it becomes possible to recover data on a failed storage device from the parities stored on the remaining healthy devices. For example, in the case of Redundant Array of Inexpensive Disks (RAID), adding one redundancy ensures data robustness to one failure, whereas adding two redundancies will provide robustness against two simultaneous failures[3]. The data durability can be further increased by implementing sophisticated erasure coding schemes[4][8] and also by prioritizing the recovery of critically damaged data stripes[9]. In all of these schemes, the durability critically depends on how fast the data on a failed device can be reconstructed to restore the redundancy. Consider the simplest case[2]: one redundancy. Given a head/drive failure, there are two paths to losing data: A second failure while recovering from a head failure: less likely due to faster recovery. A second failure while recovering from a drive failure: more likely due to longer recovery. Figure 6: Markov chain split into two parallel paths: recovery from subcomponent failures and recovery from device failures. The plot on the left shows two parallel Markov chains: Left-most shows data recovery for a ReManable failure. Right-most shows the standard recovery when a drive fails. \\(r_m\\) term represents the maximally ReManed drive population. ReManable failures on maximal drives trigger replacement. This is represented by the dashed red curves. The transition rates are time dependent, and involve \\(r_m\\) functions. \\(\\mathcal{R}^{(1)}_1(t)=exp\\Big(-\\frac{\\mathcal{C}}{2 \\Gamma}\\left[t+\\frac{1-e^{-2 \\lambda_\\mathrm{R} t}}{2 \\lambda_\\mathrm{R}}\\right]-\\frac{\\mathcal{C}}{2\\mu}\\left[ (1+2 \\kappa)t-\\frac{1-e^{-2 \\lambda_\\mathrm{R} t}}{2\\lambda_\\mathrm{R}}\\right]\\Big)\\) where \\(\\mathcal{C}\\equiv n^2\\lambda^2_{\\mathrm{R}}(1+\\kappa)\\), and \\(\\kappa\\equiv\\frac{\\lambda_\\mathrm{NR}}{\\lambda_\\mathrm{R}}\\) \\(n:\\) number of drives in the EC scheme, \\(N:\\) number of heads per drive, \\(\\Gamma:\\) ReMan Repair rate, \\(\\mu:\\) drive repair rate, \\(\\lambda_R\\): ReManable failure rate, \\(\\lambda_{NR}\\): non-ReManable failure rate. This is the expression when we allow for 1 ReMan/drive. Similar formulas are calculated for \\(R_{max}=2,3\\). We have an analytical model of Erasure Coded systems that support ReMan. The closed mathematical form can be computed instantly enabling a real-time web application. Simulation with online ReMan Below is a comparison of Monte Carlo Simulation and theoretical results[Assuming 1-ReMan/drive is allowed.]: ReManable AFR= 8% and non ReManable AFR=2%.[Taken unrealistically high to show the functional behavior] The plot shows the theoretical prediction is remarkably accurate: The gain in durability is about 4x. Figure 7: The calculated values of number of nines without ReMan(green curve), with ReMan(orange curve) and the simulation results for number of nines with ReMan (blue dots). Input parameters are taken as \\(\\lambda_\\mathrm{R}=1/12\\) and \\(\\lambda_\\mathrm{NR}=1/50\\) (1/year), which correspond to \\(8\\%\\) and \\(2\\%\\) AFR respectively. References [1] W. Padgett, Weibull distribution, 2011, pp. 16511653. [2] S. Olmez, Reliability analysis of storage systems with partially repairable devices, 2021 [Online]. Available: Accepted to be published in Transactions on Device and Materials Reliability, IEEE [3] D. A. Patterson, G. Gibson, and R. H. Katz, A case for redundant arrays of inexpensive disks (RAID), SIGMOD Rec., vol. 17, no. 3, pp. 109116, Jun. 1988, doi: 10.1145/971701.50214. [Online]. Available: https://doi.org/10.1145/971701.50214 [4] L. Hellerstein, G. A. Gibson, R. M. Karp, R. H. Katz, and D. A. Patterson, Coding techniques for handling failures in large disk arrays, Algorithmica, vol. 12, no. 2, pp. 182208, Sep. 1994, doi: 10.1007/BF01185210. [Online]. Available: https://doi.org/10.1007/BF01185210 [5] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. J. Wainwright, and K. Ramchandran, Network coding for distributed storage systems, IEEE Trans. Inf. Theor., vol. 56, no. 9, pp. 45394551, Sep. 2010, doi: 10.1109/TIT.2010.2054295. [Online]. Available: https://doi.org/10.1109/TIT.2010.2054295 [6] G. Wang, L. Xiao-Guang, and L. Jing, Parity declustering data layout for tolerating dependent disk failures in network RAID systems, 2002, pp. 2225, doi: 10.1109/ICAPP.2002.1173547. [7] V. Venkatesan and I. Iliadis, Effect of codeword placement on the reliability of erasure coded data storage systems, in Quantitative evaluation of systems, 2013, pp. 241257. [8] A. Thomasian and M. Blaum, Higher reliability redundant disk arrays: Organization, operation, and coding, ACM Trans. Storage, vol. 5, no. 3, Nov. 2009, doi: 10.1145/1629075.1629076. [Online]. Available: https://doi.org/10.1145/1629075.1629076 [9] T. Kawaguchi, Reliability analysis of distributed RAID with priority rebuilding, 2013. "]]
